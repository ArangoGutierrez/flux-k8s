# GROMACS over Kubernetes

This document describes how to run GROMACS benchmark, wrapped in a Docker image, on a Kubernetes cluster.

## Nutshell

Change directory to the `gromacs` folder in your local copy:

```bash
cd /path/to/flux-k8s/examples/gromacs
```

### Configure

Set your Kubernetes cluster:

```bash
export `KUBECONFIG`=`path/to/admin.conf`
```

Edit `conf.env` and fit the configuration to your environment.


### Generate the yaml template file (using an existing image)

To generate a manifest against the image
`quay.io/cmisale/gromacs:latest`:

```bash
make gromacs-mpijob-benchmark.yaml DOCKER_IMAGE=quay.io/cmisale/gromacs:latest
```

See below if you prefer to build your own image.

### Launch the experiments

The `run-all.sh` script will run the entire test suite with the Kubernetes Default scheduler, KubeFlux and KubeFlux with subnet awareness.

```bash
./run-all.sh
```

### Analyze the results

```bash
./stats.sh
```

## Lower-level Usage


### Generate the yaml template file (building your own image)

To build and publish your own image (e.g., `quay.io/foo:bar`) and generate a manifest against it:

```bash
make DOCKER_IMAGE_NAME=quay.io/foo DOCKER_IMAGE_TAG=bar
```

### Run tests separately

To run single tests, we need to Kustomize `kubesched.template.yaml` for KubeFlux.

The `%TAG%` variable can be changed with either `latest` or `sa` to use KubeFlux or KubeFlux with subnet awareness respectively.

For instance, to run a test suite with KubeFlux:

```bash
sed s/%TAG%/latest/g  kubesched.template.yaml > kubesched.yaml
./launch.sh flux flux kubesched.yaml
```

Or, to run KubeFlux with subnet awareness:

```bash
sed s/%TAG%/sa/g  kubesched.template.yaml > kubesched.yaml
./launch.sh flux flux-sa kubesched.yaml
```

The tests with the Kube-scheduler can be run with:

```bash
./launch.sh default default
```

More details about `launch.sh` are described in the following section.

## Details

The main script is `launch.sh` and we can use it for testing both kubeflux and the default scheduler.
Make sure you don't log in to the cluster using tokens that can expire. Exporting a KUBECONFIG is way safer.

```console
usage: ./launch.sh <flux|default> <output-file> <kubeflux-deployment-file>
```

Each run creates a `*mpi` folder, where `*` is the number of MPI ranks being launched. The output file will contain GROMACS' output.
The script looks for a template file `gromacs-mpijob-benchmark.yaml`, which is generated by `make` (see above).

This yaml has other variables that are assigned by the `launch.sh` script, such as the number of MPI ranks and changes the scheduler name.
The container image is fixed to `cmisale` quay registry. You can definitely use that container.

The script will update the `gromacs-mpijob-benchmark.yaml` and create a `deployment.yaml` file, which will be used withing the script to create and delete MPI jobs.

You will need to update the for loop in the script. It now runs `1 2 4 8 16 32 48 64` MPI ranks, 5 times for each rank, but you might need different scenarios.

### KubeFlux

To test kubeflux, we need to pass the plugin deployment file. That's because we still don't handle job cancellation and we need to reset the plugin to clean up the resource graph.

In this folder you can also find a handy deployment file for the scheduler. Please check that all things are in order with your Kube/OpenShift cluster.

```bash
./launch.sh flux flux ${PWD}/kubesched.yaml
```

### Default

To test the default scheduler, just run

```bash
./launch.sh default default
```
